{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 8: Web Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The aim of this lab is for students to get experience with **Web Mining** methods covered in week 10.\n",
    "\n",
    "#### What is Web Mining?\n",
    "Web mining is the application of data mining techniques to discover patterns and knowledge from the web. It involves extracting and analyzing information from web data sources such as web pages, web logs, and web documents.\n",
    "The primary goal of web mining is to understand customer behavior, evaluate the effectiveness of a particular website, or extract useful knowledge from the web content.\n",
    "\n",
    "#### Types of Web Mining\n",
    "1. **Web Content Mining:**\n",
    "   - **Description:** This involves mining the content of web pages. It focuses on the extraction of useful information, data, or knowledge from web page content.\n",
    "   - **Techniques Used:** Techniques include text mining, image mining, and video mining within web pages.\n",
    "   <br/><br/>\n",
    "\n",
    "2. **Web Structure Mining:**\n",
    "   - **Description:** This type examines the structure of the underlying links in the web (hyperlinks). It aims to analyze the node and connection structure of a website.\n",
    "   - **Example:** A typical application is in analyzing the link structures of top-ranking pages on search engine results.\n",
    "      <br/><br/>\n",
    "\n",
    "3. **Web Usage Mining:**\n",
    "   - **Description:** This focuses on discovering patterns from web usage data, like web server logs, browser logs, user sessions, and user queries.\n",
    "   - **Application:** It's used extensively in understanding user behavior and preferences, which is crucial for website design, e-commerce, and personalized services.\n",
    "\n",
    "#### Applications of Web Mining\n",
    "- **E-Commerce and Marketing:** Web mining helps businesses understand customer behavior, improve customer service, and target the right audience.\n",
    "- **Search Engine Optimization (SEO):** By analyzing web structure and content, businesses can optimize their sites for better visibility and ranking on search engines.\n",
    "- **Academic Research:** Researchers use web mining to analyze academic trends, publication patterns, and to gather large-scale data for analysis.\n",
    "- **Security and Surveillance:** Web mining can be used to detect unauthorized and anomalous behavior on websites.\n",
    "\n",
    "#### Challenges in Web Mining\n",
    "- **Data Volume and Complexity:** The sheer volume and complexity of web data make it challenging to extract meaningful information.\n",
    "- **Dynamic Nature of the Web:** The constantly changing nature of web content adds to the complexity of web mining.\n",
    "- **Privacy Concerns:** Ethical and privacy concerns arise when dealing with user data and personal information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping using Python\n",
    "\n",
    "Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web. Web scraping using Python is a powerful technique for programmatically collecting data from websites. This process involves writing scripts that automatically navigate web pages, extract relevant information, and store it for further analysis or use. Python, with its rich ecosystem of libraries such as BeautifulSoup, Scrapy, and Selenium, makes web scraping accessible and efficient. BeautifulSoup is widely used for parsing HTML and XML documents, allowing users to easily sift through webpage structures to extract data. Scrapy, another popular framework, provides a fast and scalable environment for crawling websites and extracting structured data. For more dynamic websites that rely heavily on JavaScript, Selenium can be used to automate browser actions, enabling the scraping of content that is loaded dynamically. The combination of these tools with Python's simplicity and readability makes web scraping a highly effective method for data collection in fields ranging from market research to academic research, where gathering large volumes of web-based information is essential. However, it's crucial to respect the legal and ethical boundaries of web scraping, adhering to a website's terms of service and using the technique responsibly to avoid overloading servers or accessing restricted information.\n",
    "\n",
    "In this lab notebook, we will be working on data extraction from the web using Python's [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) module. Please make sure to familiarise yourselves with the [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), or to refer to the documentation if you need more information for a particular class or function. The dataset to be used in the first 3 sections of the notebook is taken from a 10km race that took place in Hillsboro, USA on June 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opening HTML content\n",
    "\n",
    "Most websites are created using HTML (Hypertext Markup Language), along with CSS (Cascading Style Sheets) and JavaScript. HTML elements are separated by tags and they directly introduce content to the web page. Here is how a basic HTML document looks like: [https://www.w3schools.com/html/html_basic.asp](https://www.w3schools.com/html/html_basic.asp) - please take some time to study the link and the HTML code, we will come back into that later during the tutorial.\n",
    "\n",
    "Understanding HTML (Hypertext Markup Language) is fundamental for effective web scraping, as it is the standard language used to create web pages. HTML structures the content on the web, organizing it into a series of elements represented by tags. These tags define different types of content, such as headings, paragraphs, links, lists, and tables, making it easier to identify and extract specific data during web scraping.\n",
    "\n",
    "For web scraping, it's essential to grasp how HTML elements are nested and how they can be identified and accessed. Key elements include:\n",
    "\n",
    "1. **Tags**: The building blocks of HTML, such as `<h1>` for headings, `<p>` for paragraphs, `<a>` for hyperlinks, and `<div>` for divisions or sections of a page. Each tag typically has an opening and a closing part, like `<p>` and `</p>`.\n",
    "\n",
    "2. **Attributes**: These provide additional information about HTML elements. Common attributes include `id`, which provides a unique identifier for an element, and `class`, which classifies elements for styling and scripting purposes. In web scraping, these attributes are often used to locate specific data points.\n",
    "\n",
    "3. **DOM (Document Object Model)**: This is a programming interface for web documents. It represents the page so that programs can change the document structure, style, and content. The DOM represents the document as a tree structure wherein each node is an object representing a part of the document.\n",
    "\n",
    "4. **Tables and Lists**: Often, data on web pages is organized in tables (`<table>`, `<tr>`, `<td>`) or lists (`<ul>` for unordered lists, `<ol>` for ordered lists, and `<li>` for list items). Understanding these structures is crucial for extracting structured data.\n",
    "\n",
    "5. **Hyperlinks**: Defined with the `<a>` tag, hyperlinks are essential for web navigation and are often the target of web scraping efforts to extract URLs or follow links to other pages.\n",
    "\n",
    "A solid understanding of these HTML basics enables more efficient and targeted web scraping, allowing for the extraction of specific data from complex web pages. Tools like BeautifulSoup in Python can parse HTML and provide methods to navigate and search the DOM tree, making it easier to access and extract the desired information.\n",
    "\n",
    "We first start by loading standard python modules for data mining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform web scraping, we also import the libraries shown below. The urllib.request module is used to open URLs. The Beautiful Soup package is used to extract data from HTML files. The Beautiful Soup library's name is bs4 which stands for Beautiful Soup, version 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install beautifulsoup4\n",
    "# !pip3 install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the necessary modules, we specify the URL containing the dataset mentioned above and pass it to urlopen() to get the HTML contents of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.hubertiming.com/results/2017GPTR\"\n",
    "html = urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing HTML content \n",
    "\n",
    "Opening the HTML content of the page is just the first step. The next step is to create a Beautiful Soup object from the HTML content. This is done by passing the html content to the BeautifulSoup() function. The Beautiful Soup package is used to parse the HTML content, that is, take the raw HTML text and break it into Python objects. The second argument 'lxml' is the HTML parser whose details we do not need to worry about at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soup object allows you to extract interesting information about the website we are scraping such as getting the title of the page as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the title\n",
    "title = soup.title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the text of the webpage and quickly print it out to check if it is what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the text\n",
    "text = soup.get_text()\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the html content of the [webpage we are scraping](https://www.hubertiming.com/results/2017GPTR) by opening the webpage in another tab in a web browser, right-clicking anywhere on the webpage and selecting \"View Source\" or \"View Page Source\" (for Chrome and Firefox respectively - similar options to view the HTML source exist for other web browsers). \n",
    "\n",
    "Please take some time to inspect the html content of the webpage and spot for examples of useful tags. Examples of useful tags include < a > for hyperlinks, < table > for tables, < tr > for table rows, < th > for table headers, and < td > for table cells.\n",
    "\n",
    "We can use the find_all() method of soup to extract useful html tags within a webpage. The code below shows how to extract all the **hyperlinks** within the webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, HTML tags sometimes come with attributes such as *class* and *src*. These attributes provide additional information about html elements. We can use a for loop and the get(\"href\") method to extract and print out only hyperlinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all(\"a\")\n",
    "for link in all_links:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print out table rows only, pass the 'tr' argument in soup.find_all():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 table rows\n",
    "rows = soup.find_all('tr')  # the 'tr' tag in html denotes a table row\n",
    "print(rows[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Converting an HTML table into a Pandas dataframe\n",
    "\n",
    "The goal of this lab notebook is to take a table from a webpage and convert it into a pandas dataframe for easier manipulation using Python. For an example on how are tables encoded in HTML please study the following [example table](https://www.w3schools.com/tags/tag_tr.asp). As you'll see from the above example table, rows in HTML tables are identified using the 'tr' tag; each HTML table has a header identified by the 'th' tag; and each cell in the table is identified by the 'td' tag. Please take some time to familiarise yourselves with the example table html code in the above link.\n",
    "\n",
    "\n",
    "To convert the HTML table to a pandas dataframe, we should get all table rows in list form first and then convert that list into a dataframe. Below is a for loop that iterates through table rows and prints out the cells of the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    row_td = row.find_all('td')  # the 'td' tag in html code denotes a table cell\n",
    "    print(row_td)\n",
    "type(row_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that each row is printed with html tags embedded in each row. This is not what we want. We can remove the html tags using Beautiful Soup or regular expressions. Using regular expressions is highly discouraged since it requires several lines of code and one can easily make mistakes. It requires importing the *re* (for regular expressions) module.\n",
    "\n",
    "The easiest way to remove html tags is to use Beautiful Soup, and it takes just one line of code to do this. We pass the string of interest into BeautifulSoup() and use the get_text() method to extract the text without html tags. The following is an example of removing html tags for one row of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cells = str(row_td)\n",
    "cleantext = BeautifulSoup(str_cells, \"lxml\").get_text()\n",
    "print(cleantext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having removed HTML tags for one row, we can now convert the entire HTML table into a pandas dataframe.\n",
    "\n",
    "First, we try to scrape the header of the table, which includes names for all the table attributes. We create an empty list object, and using Beautiful Soup we locate the 'th' HTML tag which denotes a table header. We convert the header from HTML to a string, and then append it to the list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list where the table header will be stored\n",
    "header_list = []\n",
    "\n",
    "# Find the 'th' html tags which denote table header\n",
    "col_labels = soup.find_all('th')\n",
    "col_str = str(col_labels)\n",
    "cleantext_header = BeautifulSoup(col_str, \"lxml\").get_text()  # extract the text without HTML tags\n",
    "header_list.append(cleantext_header) # Add the clean table header to the list\n",
    "\n",
    "print(header_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the header above contains 9 elements, separated by commas. \n",
    "\n",
    "Now, we do the same process as above but for every row in the table that contains cell elements identified by the 'td' tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list where the table will be stored\n",
    "table_list = []\n",
    "\n",
    "# For every row in the table, find each cell element and add it to the list\n",
    "for row in rows:\n",
    "    row_td = row.find_all('td')\n",
    "    row_cells = str(row_td)\n",
    "    row_cleantext = BeautifulSoup(row_cells, \"lxml\").get_text()  # extract the text without HTML tags\n",
    "    table_list.append(row_cleantext)  # Add the clean table row to the list\n",
    "    \n",
    "print(table_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the table_list list object includes all information stored in the original table, where elements in each row are separated by commas. We also see a lot of special and uneccessary characters that would need to be removed later on.\n",
    "\n",
    "Now, we have a python list object for the header called 'header_list' and another list object for the main table called 'table_list'. We can now convert the header list into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header = pd.DataFrame(header_list)\n",
    "df_header.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is not in the format we want, since it only includes one column instead of 9 columns. To clean it up, we should split the \"0\" column into multiple columns at the comma position. This is accomplished by using the str.split() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header2 = df_header[0].str.split(',', expand=True)\n",
    "df_header2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can carry out the same process as above to convert the table list into a pandas dataframe for the table values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pd.DataFrame(table_list)\n",
    "df_table2 = df_table[0].str.split(',', expand=True)\n",
    "df_table2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better, but there is still work to do. The dataframe has unwanted square brackets surrounding each row. It also has some line and carriage return characters that can be removed (\\r, \\n). We can use the strip() method to remove the square brackets and uneccesary characters on columns 0, 1, 2 and 8. \n",
    "\n",
    "We also notice that the first few rows of thable contain overall statistics on the race, and are not formatted as the rest of the table rows, containing missing values. Therefore any rows with missing values can be removed from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uneccesary characters\n",
    "df_table2[0] = df_table2[0].str.strip('[')\n",
    "df_table2[0] = df_table2[0].str.strip(']')\n",
    "df_table2[1] = df_table2[1].str.strip(']')\n",
    "df_table2[7] = df_table2[7].str.strip(']')\n",
    "df_table2[2] = df_table2[2].str.strip('\\r\\n\\r\\n ')\n",
    "\n",
    "# Remove all rows with any missing values\n",
    "df_table3 = df_table2.dropna(axis=0, how='any')\n",
    "\n",
    "df_table3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Now we can concatenate the header dataframe with the table dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove uneccessary characters from the header\n",
    "df_header2[0] = df_header2[0].str.strip('[')\n",
    "df_header2[7] = df_header2[7].str.strip(']')\n",
    "\n",
    "# We concatenate the two dataframes\n",
    "frames = [df_header2, df_table3]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df2 = df.rename(columns=df.iloc[0]) # We assign the first row to be the dataframe header\n",
    "df3 = df2.drop(df2.index[0]) # We drop the replicated header from the first row of the dataframe\n",
    "\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! It took a while to get here, but at this point, the dataframe is in the desired format. Now the table has been both scraped from the web and has been converted into an appropriate representation where we can apply data mining operations covered through the previous lectures and labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Second example - scraping URLs\n",
    "\n",
    "For this second example, we will see how to scrape information from a fictional store, in this case a book store which is available at the following URL: [http://books.toscrape.com/](http://books.toscrape.com/)\n",
    "\n",
    "Please visit the above website, inspect the webpage, and inspect the HTML source code from your browser (using the same process described in section 2 of the lab notebook).\n",
    "\n",
    "We see that the webpage lists 20 books. For each book, there is an associated URL, which points to a separate webpage describing each book in detail. The goal of this example is to scrape the URLs for all these 20 books.\n",
    "\n",
    "We first follow the same process as in sections 1 and 2 of the lab notebook to open the URL and parse the HTML content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bookstore = \"http://books.toscrape.com/index.html\"\n",
    "html_bookstore = urlopen(url_bookstore)\n",
    "soup_bookstore = BeautifulSoup(html_bookstore, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the HTML code of the [webpage](http://books.toscrape.com/), we see that each of the 20 books is mentioned under an 'article' tag with the value 'product_pod'. Under each mention of the 'product_pod' value, there is the corresponding URL for each book, under the 'a' tag, and specifically the 'href' attribute. This seems to be a reliable source to spot product URLs.\n",
    "\n",
    "So the first thing to attempt is to find 'article' tags in the HTML code that contain the 'product_pod' value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_bookstore.find(\"article\", class_ = \"product_pod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to produce too much information. So instead of only looking for the 'product_pod' value in an 'article' tag, let's look for URLs only. \n",
    "\n",
    "If we inspect the HTML code further, we see that the URLs we are looking for are within the 'a' tag, which is within the 'div' tag. Soe we can modify the above command as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_bookstore.find(\"article\", class_ = \"product_pod\").div.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is beter, we now have all information contained within the 'a' tag for a book. But we only need the URL contained in the 'href' value, which in the above example should be \"catalogue/a-light-in-the-attic_1000/index.html\".\n",
    "\n",
    "We can get this URL by adding .get('href') to the previous instruction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_bookstore.find(\"article\", class_ = \"product_pod\").div.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now managed to get our first product URL with BeautifulSoup. Now let’s gather all the product URLs on the main web page at once using the findAll() function, which iterates across all mentions of the 'product_pod' value within an 'article' tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_urls = [x.div.a.get('href') for x in soup_bookstore.findAll(\"article\", class_ = \"product_pod\")]\n",
    "\n",
    "# Display number of fetched URLs\n",
    "print(str(len(book_urls)) + \" fetched book URLs\")\n",
    "\n",
    "# We can print all fetched URLS\n",
    "for book in book_urls:\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now managed to fetch all 20 URLs corresponding to each book in the website, and placed them in a list object in python, which can be used for further data mining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
